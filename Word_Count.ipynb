{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-ArOJL1H1Cs",
        "outputId": "190d01da-0889-4620-c511-d6f040c9d29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Count**\n",
        "\n",
        "***select()***- select column from Dataframe\n",
        "\n",
        "***Explode()***-The explode function in PySpark is used to transform a column with an array of values into multiple rows. Each row of the resulting DataFrame will contain one element of the original array column.\n",
        "\n",
        "***split()*** to split DataFrame string Column into multiple columns\n",
        "\n",
        "***alias()*** Returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode).\n",
        "\n"
      ],
      "metadata": {
        "id": "u2CtBi5-I6ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()"
      ],
      "metadata": {
        "id": "Ii1LO50nIS48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file\n",
        "text_file_path = \"/content/pyspark.txt\"\n",
        "text_data = spark.read.text(text_file_path)\n",
        "text_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWtyMsimH-f3",
        "outputId": "71a077cd-d6e9-441f-92e6-ecbcd8fa6505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|PySpark is the Py...|\n",
            "|It enables you to...|\n",
            "|It also provides ...|\n",
            "|PySpark combines ...|\n",
            "|PySpark supports ...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2= words = text_data.select(explode(split(text_data.value, \" \")).alias(\"word\"))\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_fV2j8sIbbp",
        "outputId": "d9cd08d9-ccb8-4a6a-f8cc-9831ea4ededa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|       word|\n",
            "+-----------+\n",
            "|    PySpark|\n",
            "|         is|\n",
            "|        the|\n",
            "|     Python|\n",
            "|        API|\n",
            "|        for|\n",
            "|     Apache|\n",
            "|     Spark.|\n",
            "|           |\n",
            "|         It|\n",
            "|    enables|\n",
            "|        you|\n",
            "|         to|\n",
            "|    perform|\n",
            "| real-time,|\n",
            "|large-scale|\n",
            "|       data|\n",
            "| processing|\n",
            "|         in|\n",
            "|          a|\n",
            "+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_count = df2.groupBy(\"word\").count()\n",
        "word_count.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGH7efQgJMVp",
        "outputId": "d5e6b5f8-916e-4933-e293-dfe026a4ab85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|word       |count|\n",
            "+-----------+-----+\n",
            "|API        |1    |\n",
            "|using      |1    |\n",
            "|you        |1    |\n",
            "|power      |1    |\n",
            "|DataFrames,|1    |\n",
            "|combines   |1    |\n",
            "|Streaming, |1    |\n",
            "|for        |3    |\n",
            "|provides   |1    |\n",
            "|data.      |1    |\n",
            "|in         |1    |\n",
            "|Sparkâ€™s    |1    |\n",
            "|Spark.     |1    |\n",
            "|with       |2    |\n",
            "|distributed|1    |\n",
            "|ease       |1    |\n",
            "|your       |1    |\n",
            "|analyzing  |1    |\n",
            "|such       |1    |\n",
            "|SQL,       |1    |\n",
            "+-----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}